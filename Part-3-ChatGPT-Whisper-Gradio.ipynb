{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Nutshell of ChatGPT - Part 3: Voice out an Image with ChatGPT & Whisper & Stable Diffusion\n",
    "\n",
    "12 December 2022 | by Marcus Zou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principles\n",
    "\n",
    "Basically, OpenAI's Whisper service enables a Smart Voice service that interprets the audio input into text output with the help of engine of ChatGPT. \n",
    "\n",
    "And this code piece is encouraged by .....\n",
    "\n",
    "<img src=\"resources/_006_whisper-roadmap-2-image.png\" alt=\"whisper-roadmap\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's spice 'em up!\n",
    "##### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/whisper.git\n",
    "#!pip install -q gradio\n",
    "#!pip install -q pyChatGPT\n",
    "#!pip install -q --upgrade git+https://github.com/huggingface/diffusers.git transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import gradio as gr\n",
    "from pyChatGPT import ChatGPT\n",
    "import time, warnings, torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_token =\"eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIn0..2_3T_WG9DpEqMn0_.BGGHBH8OyJBtiSsl52jXZoFD01s2mVczpt8d_gree6N0x5iNwhbpdKFCZZqR5TmsAJDmfO4dtVkgbzEQOoUriQt9A5nNjdzlPihzMtgzEVjckC9EiQdtsGNePA5qjEmQSLBNwG4LHf-U5Xa4F8F9r75dMpnKHUDkUuqvR_OMT5h3_GJb6f6axSCxwcA5Ha3rOkjfsYPXq8OPsHhTOv37Ns8jfytmAF4I0UDdwbvU3s-0p-JJym0V4OucAm8X12fDLFnTTbHBeBpWOZ3LRhkQn4MOHwnPuz0rKXzejV7dCEqbZ7btXfLc7RfGE_qFX38fPEWpNAXNY9rshGHfVvxv2857ydrzPWkpchgmTSc1u1CL0EWam_zaxk3_QXN6Ym3iYblQCwGmMr46KoititwCtXweZElJ32mfxeChAAVquwmXk6TOuUBjIskf78N84Stp_xSMiuw0bExVEnwmcb_pXurx8wlY8gEmxFZn5dPMsls6OdDg2V3v9ZIquiXeYLGmjnsT6oboCFYJKT1fr4_BkX2RIUt115-Iqa945v2Fw-FimkQFPrR8SSaBnVVSFM8gAmdxn1lmR7aaXL9ZFVw-GKlP1DFu_4C1BUeYTYsXBBbUzNh-689__VB0vO4vufhcmix6y94VDSBInjKsKILi7-wLHVIh-yNo6Xjc9G9ZX_zmu5MBu4MsE0ffhPiR7TC0_e9Yfhs_5bdwGyBAG6XlTKBJCTiM572VAWwy21FTRV3mt1rWg5lOElLnedItOHOYe0OpmtBYu33meHVpxBBSWxFWXbbPaaHFgW5c6GCWn__Ge53UeP3Jc30rA-hkHd2jTR71-O_JLP3Y8l6MT3Tvciik-tyIPrNR9Qn2COzg-cOQO_osKS86vmQqCjkJo09E8xcGgF2a-CcGgSnXCad-O5M4XhqOiwXfY6GpDOnsObxAu6pZ89FdVVDQ9Kv_WZ-g44X1ujKlR4dP1QzPb9_3dpas3KCGkGzhS38eNTmBfQX9xIJOfs_rDby9Hch6fbaBV2E1LmI7MVh41wufIxHRDYLeqVQ6y26w1u6LCWR-QufT3I9Lue4-IG9-me-yy5Vx49AB8Zi6YQC0D3TegRDRTojb-lbMAipLG2wWkfKf_XTUetfEvBofTDp3yF2mLRw-Shog2jMLHYphQgnFxeVMTJ-sjqvF1wKnBWu3ZhI84QDihatbRnM-KCH4MsWtkpB8gdLCMJvGI1BSGm08_FDmadi3Ke2jh8yeoAns80-V3WaWe154bPuPmKehtbupk0SMgVJBhOUb3XMYanuEH-l2hdxvUoibmt9__CPYYAg4eVBp2o-BV23vd48s23jQ3vFqJoQJJKBjpzN8MBoUFSpZu8joD0EDQrTmBiwwDxSkt3t_jzHwunPpx07hujJGiOj_IH_eRdPCDieqU4km7s6QfRC-QxGkkFMDV07AFBt__MvV5NpDTfxV0JQ0JiZboT9Q2yZWMFHOXwqP8i7Y8StSee8S8kc7PhsgEk1cXgPIES3P5B_YHP2OZLhJgtQPqs8PRkLkiGFL4v8sv6bWwBljXx6qrRUVWmoH2SH4_s_vUMHG2h3P93mO_4SSlxWy1yYBSDhcXrUrIHX5-qTnKg03LVDbNfO90nX_r7wwGQw-bc6E6qYCU-7f0fsAY4_bxPEFOAnLzqjilhr--9QItTX9G0i9jSt_5wngKBacY75MRlYsz3iOI5lw7oaR908Si7THjF7TDo-U7HUUu01jblgRphku7xa9Yzubpogi-iwF4fP39VpYTrceXx1elhGxaSQYnSjRpISjoAhd7JANjsYRhD3O_7_eZLs4R5srMBvPG2x07DHfB9M8rVwSOVVoGrhAbsnjx6eu7fvcrDImj0CriE-h8Vbr66Mzbznl1HDS0E0cT7Gw71cw49FYBD3c8dlwpyo-9w_XDRhSpqmhWUl--uFdE9OasqlYVpaeZ7ti6gAb9k0hha55hMXt9IOfdD7NXwV4zskKHt3cpcEU2jEar0xkY2l2FzskFdvpsN5FxRBaounK619-vJCV4AQIWlkLhM4dj9Q1iUhnGLW8DjnAeoCJEDVRLwOjBiFKijx9kzJS-8xwhY_xQaz1dFr5bk0SDj-qZFha5fozJIBInhb4qpLUunD0xWSgqI5A8GQLfpd-TSeZome7FyxsBVm57EEc9o_PvI5FisxUOuOxuBfI0XGYPOikUGxGccDuS8wCrqC2amzOVP-khH_colu2PH7NJcfH._LUjDCT0O3Bwzw4uyibDRw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base\")\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, \n",
    "                                                   subfolder=\"scheduler\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, \n",
    "                                               scheduler=scheduler,\n",
    "                                               revision=\"fp16\", \n",
    "                                               torch_dtype=torch.float16)\n",
    "\n",
    "pipe =pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transcribe Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(audio):\n",
    "\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    audio = whisper.load_audio(audio)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    # make low-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "    # detect the spoken language\n",
    "    _, probs = model.detect_language(mel)\n",
    "\n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions()\n",
    "    result = whisper.decode(model, mel, options)\n",
    "    result_text = result.text\n",
    "\n",
    "    # pass the generated text to audio\n",
    "    chatgpt_api = ChatGPT(session_token)\n",
    "    response = chatgpt_api.send_message(result_text)\n",
    "    output_result = response[\"message\"]\n",
    "\n",
    "    output_image = pipe(output_result, height=768, width=768).images[0]\n",
    "\n",
    "    return [result_text, output_result, output_image]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = gr.Textbox(label=\"Speech to Text\")\n",
    "output_2 = gr.Textbox(label=\"ChatGPT Output\")\n",
    "output_3 = gr.Image(label=\"Diffusion Output\")\n",
    "\n",
    "gr.Interface(\n",
    "    title = 'OpenAI Whisper/ChatGPT/Diffusion ASR Web UI',\n",
    "    fn = transcribe,\n",
    "    inputs = [\n",
    "        gr.inputs.Audio(source = \"microphone\", type=\"filepath\")\n",
    "    ],\n",
    "    outputs = [\n",
    "        output_1, output_2, output_3\n",
    "    ],\n",
    "    live=True,\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "fdb3b0d5e471e949725138a1ef716dcf553b73f8a3243b6f92baf3fc7cfc3d2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
